{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBDT_LR (CTR预估 + Churn rate预测).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1MmMyjn7pVj7jzUyoJ8s_k7aSVBppAnGB",
      "authorship_tag": "ABX9TyMPtJrmi9t2veo5GAGUyavG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/dl-recomm-algo-implementation/blob/main/GBDT_LR_(CTR%E9%A2%84%E4%BC%B0_%2B_Churn_rate%E9%A2%84%E6%B5%8B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-yL44EP4Mr0"
      },
      "source": [
        "![](https://pic4.zhimg.com/80/v2-7fe5861dd26ef82c3e79f25f46c4ce83_1440w.jpg)\n",
        "\n",
        "图中Tree1、Tree2为通过GBDT模型学出来的两颗树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。由于树的每条路径，是通过最小化均方差等方法最终分割出来的有区分性路径，根据该路径得到的特征、特征组合都相对有区分性，效果理论上不会亚于人工经验的处理方式。\n",
        "\n",
        "GBDT模型的特点，非常适合用来挖掘有效的特征、特征组合。业界不仅GBDT+LR融合有实践，GBDT+FM也有实践，2014 Kaggle CTR竞赛冠军就是使用GBDT+FM，可见，使用GBDT融合其它模型是非常值得尝试的思路。\n",
        "\n",
        "调研了Facebook、Kaggle竞赛关于GBDT建树的细节，发现两个关键点：采用ensemble决策树而非单颗树；建树采用GBDT而非RF（Random Forests）。解读如下：\n",
        "\n",
        "1） 为什么建树采用ensemble决策树？\n",
        "\n",
        "  一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在\n",
        "  学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，\n",
        "  多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。\n",
        "\n",
        "  2） 为什么建树采用GBDT而非RF？\n",
        "\n",
        "  RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，\n",
        "  主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，\n",
        "  思路更加合理，这应该也是用GBDT的原因。\n",
        "\n",
        "  **GBDT + LR不只是可以用来做CTR预估(根据LR的概率排序)，也可以应用到寻常的Classification模型中，下文我们给出了两例**："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4vBObq8738s"
      },
      "source": [
        "# Case 1: Porto Seguro’s Safe Driver Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQT6ZGZgAKRN"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqKgbxxkEmaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a66019-0bd0-4a98-92a5-cb7f905d7c9c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "TRAINING_PATH = '/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/train.csv'\n",
        "TESTING_PATH  = '/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/test.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCr0F5DPAYw_",
        "outputId": "4119c035-8880-4b99-8329-148cfa090c79"
      },
      "source": [
        "pd.read_csv(TRAINING_PATH).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(595212, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kCuMoTIRhNB"
      },
      "source": [
        "In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., `ind`, `reg`, `car`, `calc`). In addition, feature names include the postfix `bin` to indicate binary features and `cat` to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HifC0db_-zKO",
        "outputId": "db3e1c21-e65a-4eac-87ed-a67a8a2ee02f"
      },
      "source": [
        "print('Load data...')\n",
        "df_train = pd.read_csv(TRAINING_PATH).iloc[:10000]\n",
        "df_test  = pd.read_csv(TRAINING_PATH).iloc[10000:12000]\n",
        "\n",
        "NUMERIC_COLS = [\n",
        "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
        "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
        "]\n",
        "\n",
        "\n",
        "print(df_train.head(3))\n",
        "print(df_test.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "   id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "0   7       0          2  ...               0               0               1\n",
            "1   9       0          1  ...               0               1               0\n",
            "2  13       0          5  ...               0               1               0\n",
            "\n",
            "[3 rows x 59 columns]\n",
            "          id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "10000  25242       0          0  ...               1               0               0\n",
            "10001  25246       0          1  ...               1               1               0\n",
            "10002  25247       0          0  ...               1               0               0\n",
            "\n",
            "[3 rows x 59 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnQTwQH8AouZ"
      },
      "source": [
        "y_train = df_train['target']  # training label\n",
        "y_test = df_test['target']  # testing label\n",
        "X_train = df_train[NUMERIC_COLS]  # training dataset\n",
        "X_test = df_test[NUMERIC_COLS]  # testing dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "cWbYOEuhO8dM",
        "outputId": "cfb709f2-4cca-4012-9fcd-3c2006b4228d"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>ps_ind_01</th>\n",
              "      <th>ps_ind_02_cat</th>\n",
              "      <th>ps_ind_03</th>\n",
              "      <th>ps_ind_04_cat</th>\n",
              "      <th>ps_ind_05_cat</th>\n",
              "      <th>ps_ind_06_bin</th>\n",
              "      <th>ps_ind_07_bin</th>\n",
              "      <th>ps_ind_08_bin</th>\n",
              "      <th>ps_ind_09_bin</th>\n",
              "      <th>ps_ind_10_bin</th>\n",
              "      <th>ps_ind_11_bin</th>\n",
              "      <th>ps_ind_12_bin</th>\n",
              "      <th>ps_ind_13_bin</th>\n",
              "      <th>ps_ind_14</th>\n",
              "      <th>ps_ind_15</th>\n",
              "      <th>ps_ind_16_bin</th>\n",
              "      <th>ps_ind_17_bin</th>\n",
              "      <th>ps_ind_18_bin</th>\n",
              "      <th>ps_reg_01</th>\n",
              "      <th>ps_reg_02</th>\n",
              "      <th>ps_reg_03</th>\n",
              "      <th>ps_car_01_cat</th>\n",
              "      <th>ps_car_02_cat</th>\n",
              "      <th>ps_car_03_cat</th>\n",
              "      <th>ps_car_04_cat</th>\n",
              "      <th>ps_car_05_cat</th>\n",
              "      <th>ps_car_06_cat</th>\n",
              "      <th>ps_car_07_cat</th>\n",
              "      <th>ps_car_08_cat</th>\n",
              "      <th>ps_car_09_cat</th>\n",
              "      <th>ps_car_10_cat</th>\n",
              "      <th>ps_car_11_cat</th>\n",
              "      <th>ps_car_11</th>\n",
              "      <th>ps_car_12</th>\n",
              "      <th>ps_car_13</th>\n",
              "      <th>ps_car_14</th>\n",
              "      <th>ps_car_15</th>\n",
              "      <th>ps_calc_01</th>\n",
              "      <th>ps_calc_02</th>\n",
              "      <th>ps_calc_03</th>\n",
              "      <th>ps_calc_04</th>\n",
              "      <th>ps_calc_05</th>\n",
              "      <th>ps_calc_06</th>\n",
              "      <th>ps_calc_07</th>\n",
              "      <th>ps_calc_08</th>\n",
              "      <th>ps_calc_09</th>\n",
              "      <th>ps_calc_10</th>\n",
              "      <th>ps_calc_11</th>\n",
              "      <th>ps_calc_12</th>\n",
              "      <th>ps_calc_13</th>\n",
              "      <th>ps_calc_14</th>\n",
              "      <th>ps_calc_15_bin</th>\n",
              "      <th>ps_calc_16_bin</th>\n",
              "      <th>ps_calc_17_bin</th>\n",
              "      <th>ps_calc_18_bin</th>\n",
              "      <th>ps_calc_19_bin</th>\n",
              "      <th>ps_calc_20_bin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.718070</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.883679</td>\n",
              "      <td>0.370810</td>\n",
              "      <td>3.605551</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.766078</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>0.316228</td>\n",
              "      <td>0.618817</td>\n",
              "      <td>0.388716</td>\n",
              "      <td>2.449490</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0.316228</td>\n",
              "      <td>0.641586</td>\n",
              "      <td>0.347275</td>\n",
              "      <td>3.316625</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.580948</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>104</td>\n",
              "      <td>1</td>\n",
              "      <td>0.374166</td>\n",
              "      <td>0.542949</td>\n",
              "      <td>0.294958</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.840759</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>0.316070</td>\n",
              "      <td>0.565832</td>\n",
              "      <td>0.365103</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
              "0   7       0          2  ...               0               0               1\n",
              "1   9       0          1  ...               0               1               0\n",
              "2  13       0          5  ...               0               1               0\n",
              "3  16       0          0  ...               0               0               0\n",
              "4  17       0          0  ...               1               1               0\n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7og5BCzvOmg"
      },
      "source": [
        "训练GBDT模型\n",
        "\n",
        "本文使用lightgbm包来训练我们的GBDT模型，每个case训练100 trees，每棵树有64个叶子结点 (leaves)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYuspr8zEmNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a1ac21-5a2d-41a2-a5f2-217fa8c5f3d9"
      },
      "source": [
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'binary_logloss'},\n",
        "    'num_trees': 100,  # 每个case总共100棵树\n",
        "    'num_leaves': 64,   # 每棵树有64个leaves\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# number of leaves,will be used in feature transformation\n",
        "num_leaf = 64\n",
        "\n",
        "print('Start training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=100,\n",
        "                valid_sets=lgb_train,\n",
        "                verbose_eval = False\n",
        "                )\n",
        "\n",
        "print('Save model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Save model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7f2bf0a699d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91cb0M_GvuGG"
      },
      "source": [
        "特征转换\n",
        "\n",
        "在训练得到100棵树之后，我们需要得到的不是GBDT的预测结果，而是每一条训练数据落在了每棵树的哪个叶子结点上，因此需要使用下面的语句："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coagmzackiRb",
        "outputId": "88c4f541-4546-40e9-d0d0-342d8524c8ed"
      },
      "source": [
        "print('Start predicting...')\n",
        "# predict and get data on leaves, training data\n",
        "y_pred = gbm.predict(X_train, pred_leaf=True)\n",
        "\n",
        "print(np.array(y_pred).shape)\n",
        "print(y_pred[0])\n",
        "print(y_pred[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start predicting...\n",
            "(10000, 100)\n",
            "[28 59 17 50 31 22 18 40 22 40 61  7  7  7 53  3 47 22 23 35 41 34 33 43\n",
            " 50 46 19 42 44 48 42 30 10  9  9 63 21 61 46 58 23 37 50  6 56 35 34 38\n",
            " 36 40 34 40 30 30 36 52 29 54 35 62  0 48 52 37 55  1 57 61 45 57 36 53\n",
            " 36 36 14 54 56 40 38 38  1  1 30  1 38 62 25 31 29 23 29 40 18 41 40  1\n",
            " 42 52 48 45]\n",
            "(100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_nUyjsvx07"
      },
      "source": [
        "打印上面结果的输出，可以看到shape是[10000,100]，即`[data size * #trees per case]`\n",
        "\n",
        "然后我们需要将每棵树的特征进行one-hot处理，如前面所说，总共100棵树，假设第一棵树落在28号leaf node上，那我们需要建立一个64维的向量(因为有64棵树)，除43维之外全部都是0。因此用于LR训练的特征维数共`[num_trees * num_leaves]`，也就是100个64维的one-hot vector。\n",
        "\n",
        "这样就为每个data point创建了`[100*64]`的新特征向量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko_tgdLSEmKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd3632f-b5fb-4b50-e47b-5198c5af0ea0"
      },
      "source": [
        "print('Writing transformed training data')\n",
        "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
        "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
        "\n",
        "print(transformed_training_matrix[0].shape)   \n",
        "\n",
        "for i in range(0, len(y_pred)):\n",
        "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
        "    transformed_training_matrix[i][temp] += 1\n",
        "\n",
        "\n",
        "y_pred = gbm.predict(X_test, pred_leaf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing transformed training data\n",
            "(6400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBGpaPL2mup"
      },
      "source": [
        "每个data point transform为为`[100*64]`的新特征向量。当然，对于测试集也要进行同样的处理.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwqARpXTEmHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835303df-1cd1-4425-8120-f7d58da21d2c"
      },
      "source": [
        "print('Writing transformed testing data')\n",
        "transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
        "for i in range(0, len(y_pred)):\n",
        "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
        "    transformed_testing_matrix[i][temp] += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing transformed testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxePwtlA3ITR"
      },
      "source": [
        "LR训练\n",
        "\n",
        "然后我们可以用转换后的训练集特征和label训练我们的LR模型，并对测试集进行测试："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SXMfyDQ3H9v",
        "outputId": "090d5035-e170-400c-a875-a50d03070e79"
      },
      "source": [
        "lm = LogisticRegression(penalty='l2',C=0.05) # logestic model construction\n",
        "lm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
        "y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
        "\n",
        "print(y_pred_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.97742378 0.02257622]\n",
            " [0.98815137 0.01184863]\n",
            " [0.98046795 0.01953205]\n",
            " ...\n",
            " [0.98790653 0.01209347]\n",
            " [0.99404478 0.00595522]\n",
            " [0.97347545 0.02652455]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izxaS_6y3WNM"
      },
      "source": [
        "我们这里得到的不是简单的类别，而是每个类别的概率, 我们需要对这样的类别概率进行排序，得到我们的TopN。\n",
        "\n",
        "效果评价\n",
        "在Facebook的paper中，模型使用NE(Normalized Cross-Entropy)，进行评价，计算公式如下:\n",
        "\n",
        "![](https://pic2.zhimg.com/80/v2-b2a88a7874e01b316d10a31aa3863171_1440w.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD2-9FsmEmEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415ad1cb-e8fb-4950-ddf5-6b2d50696db3"
      },
      "source": [
        "NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))\n",
        "print(\"Normalized Cross Entropy \" + str(NE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized Cross Entropy 2.1481980203420465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQzfr6vEmAu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMPKvdDeEl9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBziDP677bL"
      },
      "source": [
        "# Case 2: 基于LR+XGBoost预估电信客户流失\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHJYGPuEl6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c2e2f1-f361-4a0e-fccc-4e31a5cb2935"
      },
      "source": [
        "# -*-coding:utf-8-*-\n",
        "\"\"\"\n",
        "    Author: Alan\n",
        "    Desc:\n",
        "        GBDT+LR模型 电信客户流失预测\n",
        "\"\"\"\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class ChurnPredWithGBDTAndLR:\n",
        "    def __init__(self):\n",
        "        self.file = \"/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/new_churn.csv\"\n",
        "        self.data = self.load_data()\n",
        "        self.train, self.test = self.split()\n",
        "\n",
        "    # 加载数据\n",
        "    def load_data(self):\n",
        "        return pd.read_csv(self.file)\n",
        "\n",
        "    # 拆分数据集\n",
        "    def split(self):\n",
        "        train, test = train_test_split(self.data, test_size=0.1, random_state=40)\n",
        "        return train, test\n",
        "\n",
        "    # 模型训练\n",
        "    def train_model(self):\n",
        "        lable = \"Churn\"\n",
        "        ID = \"customerID\"\n",
        "        x_columns = [x for x in self.train.columns if x not in [lable, ID]]\n",
        "        x_train = self.train[x_columns]\n",
        "        y_train = self.train[lable]\n",
        "\n",
        "        # 创建gbdt模型 并训练\n",
        "        gbdt = GradientBoostingClassifier()\n",
        "        gbdt.fit(x_train, y_train)\n",
        "\n",
        "        # 模型融合\n",
        "        gbdt_lr = LogisticRegression()\n",
        "        enc = OneHotEncoder()\n",
        "        print(gbdt.apply(x_train).shape)\n",
        "        print(gbdt.apply(x_train).reshape(-1,100).shape)\n",
        "\n",
        "        # 100为n_estimators，迭代次数\n",
        "        enc.fit(gbdt.apply(x_train).reshape(-1,100))\n",
        "        gbdt_lr.fit(enc.transform(gbdt.apply(x_train).reshape(-1,100)),y_train)\n",
        "\n",
        "        return enc, gbdt, gbdt_lr\n",
        "\n",
        "    # 效果评估\n",
        "    def evaluate(self,enc,gbdt,gbdt_lr):\n",
        "        lable = \"Churn\"\n",
        "        ID = \"customerID\"\n",
        "        x_columns = [x for x in self.test.columns if x not in [lable, ID]]\n",
        "        x_test = self.test[x_columns]\n",
        "        y_test = self.test[lable]\n",
        "\n",
        "        # gbdt 模型效果评估\n",
        "        gbdt_y_pred = gbdt.predict_proba(x_test)\n",
        "        new_gbdt_y_pred = list()\n",
        "        for y in gbdt_y_pred:\n",
        "            # y[0] 表示样本label=0的概率 y[1]表示样本label=1的概率\n",
        "            new_gbdt_y_pred.append(1 if y[1] > 0.5 else 0)\n",
        "        print(\"GBDT-MSE: %.4f\" % mean_squared_error(y_test, new_gbdt_y_pred))\n",
        "        print(\"GBDT-Accuracy : %.4g\" % metrics.accuracy_score(y_test.values, new_gbdt_y_pred))\n",
        "        print(\"GBDT-AUC Score : %.4g\" % metrics.roc_auc_score(y_test.values, new_gbdt_y_pred))\n",
        "\n",
        "        gbdt_lr_y_pred = gbdt_lr.predict_proba(enc.transform(gbdt.apply(x_test).reshape(-1,100)))\n",
        "        new_gbdt_lr_y_pred = list()\n",
        "        for y in gbdt_lr_y_pred:\n",
        "            # y[0] 表示样本label=0的概率 y[1]表示样本label=1的概率\n",
        "            new_gbdt_lr_y_pred.append(1 if y[1] > 0.5 else 0)\n",
        "        print(\"GBDT_LR-MSE: %.4f\" % mean_squared_error(y_test, new_gbdt_lr_y_pred))\n",
        "        print(\"GBDT_LR-Accuracy : %.4g\" % metrics.accuracy_score(y_test.values, new_gbdt_lr_y_pred))\n",
        "        print(\"GBDT_LR-AUC Score : %.4g\" % metrics.roc_auc_score(y_test.values, new_gbdt_lr_y_pred))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pred = ChurnPredWithGBDTAndLR()\n",
        "    enc, gbdt, gbdt_lr = pred.train_model()\n",
        "    pred.evaluate(enc, gbdt,gbdt_lr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6338, 100, 1)\n",
            "(6338, 100)\n",
            "GBDT-MSE: 0.2199\n",
            "GBDT-Accuracy : 0.7801\n",
            "GBDT-AUC Score : 0.7058\n",
            "GBDT_LR-MSE: 0.2638\n",
            "GBDT_LR-Accuracy : 0.7362\n",
            "GBDT_LR-AUC Score : 0.6647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ9P1xJNSq5P"
      },
      "source": [
        "# Case 3: Criteo 项目 (失败)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xYATHORu8Ga",
        "outputId": "ca84ef0e-bb48-4c89-c8ac-9d9a01843551"
      },
      "source": [
        "!git clone https://github.com/chengstone/kaggle_criteo_ctr_challenge-.git"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'kaggle_criteo_ctr_challenge-'...\n",
            "remote: Enumerating objects: 156, done.\u001b[K\n",
            "remote: Total 156 (delta 0), reused 0 (delta 0), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (156/156), 2.10 MiB | 23.42 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emGKLinOha1f"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import click\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtsbrLfVSqfO",
        "outputId": "0e7e5533-38e8-4487-b8bc-9e982842f227"
      },
      "source": [
        "!wget --no-check-certificate https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10082655/dac.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-09 23:49:08--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/10082655/dac.tar.gz\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.112.35\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.112.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4576820670 (4.3G) [binary/octet-stream]\n",
            "Saving to: ‘dac.tar.gz’\n",
            "\n",
            "dac.tar.gz          100%[===================>]   4.26G  33.2MB/s    in 2m 13s  \n",
            "\n",
            "2021-07-09 23:51:21 (32.8 MB/s) - ‘dac.tar.gz’ saved [4576820670/4576820670]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIFFSblFU_Lo",
        "outputId": "6af9303c-079a-4ec6-9c05-ec5c9fdddcb4"
      },
      "source": [
        "!tar zxf dac.tar.gz\n",
        "\n",
        "!rm -f dac.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43WH0YZg77Sd"
      },
      "source": [
        "!mkdir raw\n",
        "\n",
        "!mv ./*.txt raw/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr10ZhdLtdtn"
      },
      "source": [
        "!mkdir raw2\n",
        "\n",
        "#!mv ./*.txt raw2/"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrGmziuCTHTN"
      },
      "source": [
        "# 选取前1000000样本\n",
        "\n",
        "# !head -n 1000 Criteo/test.txt > test_sub100w.txt\n",
        "\n",
        "# !head -n 1000 Criteo/train.txt > train_sub100w.txt\n",
        "\n",
        "!head -n 100000 raw/test.txt > raw2/test.txt\n",
        "\n",
        "!head -n 100000 raw/train.txt > raw2/train.txt"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CDL0jzDhhZY"
      },
      "source": [
        "import pickle\n",
        "\n",
        "def save_params(params):\n",
        "    \"\"\"\n",
        "    Save parameters to file\n",
        "    \"\"\"\n",
        "    pickle.dump(params, open('params.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "    return pickle.load(open('params.p', mode='rb'))\n",
        "\n",
        "\n",
        "def save_params_with_name(params, name):\n",
        "    \"\"\"\n",
        "    Save parameters to file\n",
        "    \"\"\"\n",
        "    pickle.dump(params, open('{}.p'.format(name), 'wb'))\n",
        "\n",
        "\n",
        "def load_params_with_name(name):\n",
        "    \"\"\"\n",
        "    Load parameters from file\n",
        "    \"\"\"\n",
        "    return pickle.load(open('{}.p'.format(name), mode='rb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmRPttm_hhVc"
      },
      "source": [
        "# There are 13 integer features and 26 categorical features\n",
        "continous_features = range(1, 14)\n",
        "categorial_features = range(14, 40)\n",
        "\n",
        "# Clip integer features. The clip point for each integer feature\n",
        "# is derived from the 95% quantile of the total values in each feature\n",
        "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
        "\n",
        "class ContinuousFeatureGenerator:\n",
        "    \"\"\"\n",
        "    Normalize the integer features to [0, 1] by min-max normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feature):\n",
        "        self.num_feature = num_feature\n",
        "        self.min = [sys.maxsize] * num_feature\n",
        "        self.max = [-sys.maxsize] * num_feature\n",
        "\n",
        "    def build(self, datafile, continous_features):\n",
        "        with open(datafile, 'r') as f:\n",
        "            for line in f:\n",
        "                features = line.rstrip('\\n').split('\\t')\n",
        "                for i in range(0, self.num_feature):\n",
        "                    val = features[continous_features[i]]\n",
        "                    if val != '':\n",
        "                        val = int(val)\n",
        "                        if val > continous_clip[i]:\n",
        "                            val = continous_clip[i]\n",
        "                        self.min[i] = min(self.min[i], val)\n",
        "                        self.max[i] = max(self.max[i], val)\n",
        "\n",
        "    def gen(self, idx, val):\n",
        "        if val == '':\n",
        "            return 0.0\n",
        "        val = float(val)\n",
        "        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n",
        "\n",
        "class CategoryDictGenerator:\n",
        "    \"\"\"\n",
        "    Generate dictionary for each of the categorical features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feature):\n",
        "        self.dicts = []\n",
        "        self.num_feature = num_feature\n",
        "        for i in range(0, num_feature):\n",
        "            self.dicts.append(collections.defaultdict(int))\n",
        "\n",
        "    def build(self, datafile, categorial_features, cutoff=0):\n",
        "        with open(datafile, 'r') as f:\n",
        "            for line in f:\n",
        "                features = line.rstrip('\\n').split('\\t')\n",
        "                for i in range(0, self.num_feature):\n",
        "                    if features[categorial_features[i]] != '':\n",
        "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
        "        for i in range(0, self.num_feature):\n",
        "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
        "                                   self.dicts[i].items())\n",
        "\n",
        "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
        "            vocabs, _ = list(zip(*self.dicts[i]))\n",
        "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
        "            self.dicts[i]['<unk>'] = 0\n",
        "\n",
        "    def gen(self, idx, key):\n",
        "        if key not in self.dicts[idx]:\n",
        "            res = self.dicts[idx]['<unk>']\n",
        "        else:\n",
        "            res = self.dicts[idx][key]\n",
        "        return res\n",
        "\n",
        "    def dicts_sizes(self):\n",
        "        return list(map(len, self.dicts))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztY_n5bZhhSs"
      },
      "source": [
        "def preprocess(datadir, outdir):\n",
        "    \"\"\"\n",
        "    All the 13 integer features are normalzied to continous values and these\n",
        "    continous features are combined into one vecotr with dimension 13.\n",
        "\n",
        "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
        "    vectors are combined into one sparse binary vector.\n",
        "    \"\"\"\n",
        "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
        "    dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
        "\n",
        "    dicts = CategoryDictGenerator(len(categorial_features))\n",
        "    dicts.build(\n",
        "        os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)#200 50\n",
        "\n",
        "    dict_sizes = dicts.dicts_sizes()\n",
        "    categorial_feature_offset = [0]\n",
        "    for i in range(1, len(categorial_features)):\n",
        "        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
        "        categorial_feature_offset.append(offset)\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    print('training set process started')\n",
        "\n",
        "    # 90% of the data are used for training, and 10% of the data are used\n",
        "    # for validation.\n",
        "    train_ffm = open(os.path.join(outdir, 'train_ffm.txt'), 'w')\n",
        "    valid_ffm = open(os.path.join(outdir, 'valid_ffm.txt'), 'w')\n",
        "\n",
        "    train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
        "    valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
        "\n",
        "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
        "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
        "            with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
        "                for line in f:\n",
        "                    features = line.rstrip('\\n').split('\\t')\n",
        "                    continous_feats = []\n",
        "                    continous_vals = []\n",
        "                    for i in range(0, len(continous_features)):\n",
        "\n",
        "                        val = dists.gen(i, features[continous_features[i]])\n",
        "                        continous_vals.append(\n",
        "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
        "                        continous_feats.append(\n",
        "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
        "\n",
        "                    categorial_vals = []\n",
        "                    categorial_lgb_vals = []\n",
        "                    for i in range(0, len(categorial_features)):\n",
        "                        val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
        "                        categorial_vals.append(str(val))\n",
        "                        val_lgb = dicts.gen(i, features[categorial_features[i]])\n",
        "                        categorial_lgb_vals.append(str(val_lgb))\n",
        "\n",
        "                    continous_vals = ','.join(continous_vals)\n",
        "                    categorial_vals = ','.join(categorial_vals)\n",
        "                    label = features[0]\n",
        "                    if random.randint(0, 9999) % 10 != 0:\n",
        "                        out_train.write(','.join(\n",
        "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
        "                        train_ffm.write('\\t'.join(label) + '\\t')\n",
        "                        train_ffm.write('\\t'.join(\n",
        "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
        "                        train_ffm.write('\\t'.join(\n",
        "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
        "                        \n",
        "                        train_lgb.write('\\t'.join(label) + '\\t')\n",
        "                        train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
        "                        train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
        "\n",
        "                    else:\n",
        "                        out_valid.write(','.join(\n",
        "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
        "                        valid_ffm.write('\\t'.join(label) + '\\t')\n",
        "                        valid_ffm.write('\\t'.join(\n",
        "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
        "                        valid_ffm.write('\\t'.join(\n",
        "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
        "                                                \n",
        "                        valid_lgb.write('\\t'.join(label) + '\\t')\n",
        "                        valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
        "                        valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
        "                        \n",
        "    print('training set process finished')\n",
        "\n",
        "    train_ffm.close()\n",
        "    valid_ffm.close()\n",
        "\n",
        "    train_lgb.close()\n",
        "    valid_lgb.close()\n",
        "\n",
        "    print('testing set process started')\n",
        "\n",
        "    test_ffm = open(os.path.join(outdir, 'test_ffm.txt'), 'w')\n",
        "    test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
        "\n",
        "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
        "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                features = line.rstrip('\\n').split('\\t')\n",
        "\n",
        "                continous_feats = []\n",
        "                continous_vals = []\n",
        "                for i in range(0, len(continous_features)):\n",
        "                    val = dists.gen(i, features[continous_features[i] - 1])\n",
        "                    continous_vals.append(\n",
        "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
        "                    continous_feats.append(\n",
        "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
        "\n",
        "                categorial_vals = []\n",
        "                categorial_lgb_vals = []\n",
        "                for i in range(0, len(categorial_features)):\n",
        "                    val = dicts.gen(i,\n",
        "                                    features[categorial_features[i] -\n",
        "                                             1]) + categorial_feature_offset[i]\n",
        "                    categorial_vals.append(str(val))\n",
        "\n",
        "                    val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
        "                    categorial_lgb_vals.append(str(val_lgb))\n",
        "\n",
        "                continous_vals = ','.join(continous_vals)\n",
        "                categorial_vals = ','.join(categorial_vals)\n",
        "\n",
        "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
        "                \n",
        "                test_ffm.write('\\t'.join(['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
        "                test_ffm.write('\\t'.join(\n",
        "                    ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
        "                                                                \n",
        "                test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
        "                test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
        "\n",
        "    test_ffm.close()\n",
        "    test_lgb.close()\n",
        "\n",
        "    print('testing set process finished')\n",
        "\n",
        "    return dict_sizes"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8BO-_ByhhQg",
        "outputId": "5e1d43f4-97ae-4b1d-aec1-eb0ca84e2c6b"
      },
      "source": [
        "!mkdir data\n",
        "\n",
        "dict_sizes = preprocess('./raw2','./data')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set process started\n",
            "training set process finished\n",
            "testing set process started\n",
            "testing set process finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu3bj21Qhp9O"
      },
      "source": [
        "save_params_with_name((dict_sizes), 'dict_sizes') #pickle.dump((dict_sizes), open('dict_sizes.p', 'wb'))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM2L5orVhp5-"
      },
      "source": [
        "dict_sizes = load_params_with_name('dict_sizes') #pickle.load(open('dict_sizes.p', mode='rb'))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfj66lwzhp2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1de2e80-0197-4c37-fe34-ba356de60d1c"
      },
      "source": [
        "sum(dict_sizes)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Jo9L_euFTY"
      },
      "source": [
        "## FFM training\n",
        "\n",
        "FFM input data is prepared, lets get started training FFM model.\n",
        "\n",
        " - learning ratee = 0.1\n",
        " - epoch = 32\n",
        " - model saved as `model_ffm`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92vlXavkhp0l"
      },
      "source": [
        "import subprocess, sys, os, time\n",
        "\n",
        "NR_THREAD = 1"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTQiKnwAhpxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af595e4e-cbdc-41f6-ae4b-6df529abed5a"
      },
      "source": [
        "cmd = '/content/kaggle_criteo_ctr_challenge-/libffm/libffm/ffm-train --auto-stop -r 0.1 -t 32 -s {nr_thread} -p ./data/valid_ffm.txt ./data/train_ffm.txt model_ffm'.format(nr_thread=NR_THREAD) \n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpDUsYRAhpuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ba62bb-6521-46f9-ccad-583693387852"
      },
      "source": [
        "cmd = '/content/kaggle_criteo_ctr_challenge-/libffm/libffm/ffm-predict ./data/train_ffm.txt model_ffm tr_ffm.out'.format(nr_thread=NR_THREAD) \n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9ZXEJMZhpr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdc3518-30e5-4e93-8e80-5fa992d691ce"
      },
      "source": [
        "cmd = '/content/kaggle_criteo_ctr_challenge-/libffm/libffm/ffm-predict ./data/valid_ffm.txt model_ffm va_ffm.out'.format(nr_thread=NR_THREAD) \n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOUcm2msw292",
        "outputId": "5c9f3f74-17e3-4202-e6a7-53d7ea016bf2"
      },
      "source": [
        "cmd = '/content/kaggle_criteo_ctr_challenge-/libffm/libffm/ffm-predict ./data/test_ffm.txt model_ffm te_ffm.out true'.format(nr_thread=NR_THREAD) \n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEJ9yoraw27B"
      },
      "source": [
        "def lgb_pred(tr_path, va_path, _sep = '\\t', iter_num = 32):\n",
        "    # load or create your dataset\n",
        "    print('Load data...')\n",
        "    df_train = pd.read_csv(tr_path, header=None, sep=_sep)\n",
        "    df_test = pd.read_csv(va_path, header=None, sep=_sep)\n",
        "    \n",
        "    y_train = df_train[0].values\n",
        "    y_test = df_test[0].values\n",
        "    X_train = df_train.drop(0, axis=1).values\n",
        "    X_test = df_test.drop(0, axis=1).values\n",
        "    \n",
        "    # create dataset for lightgbm\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "    \n",
        "    # specify your configurations as a dict\n",
        "    params = {\n",
        "        'task': 'train',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': {'l2', 'auc', 'logloss'},\n",
        "        'num_leaves': 30,\n",
        "#         'max_depth': 7,\n",
        "        'num_trees': 32,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': 0\n",
        "    }\n",
        "    \n",
        "    print('Start training...')\n",
        "    # train\n",
        "    gbm = lgb.train(params,\n",
        "                    lgb_train,\n",
        "                    num_boost_round=iter_num,\n",
        "                    valid_sets=lgb_eval,\n",
        "                    feature_name=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"],\n",
        "                    categorical_feature=[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"],\n",
        "                    early_stopping_rounds=5)\n",
        "    \n",
        "    print('Save model...')\n",
        "    # save model to file\n",
        "    gbm.save_model('lgb_model.txt')\n",
        "    \n",
        "    print('Start predicting...')\n",
        "    # predict\n",
        "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "    # eval\n",
        "    print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
        "\n",
        "    return gbm,y_pred,X_train,y_train"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfqLuGSrw25R",
        "outputId": "c9b9579d-7fc3-41a2-fa5f-3997c2ab37ad"
      },
      "source": [
        "gbm,y_pred,X_train ,y_train = lgb_pred('./data/train_lgb.txt', './data/valid_lgb.txt', '\\t', 256)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
            "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
            "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1]\tvalid_0's auc: 0.708795\tvalid_0's l2: 0.172283\n",
            "Training until validation scores don't improve for 5 rounds.\n",
            "[2]\tvalid_0's auc: 0.719777\tvalid_0's l2: 0.170605\n",
            "[3]\tvalid_0's auc: 0.72299\tvalid_0's l2: 0.168996\n",
            "[4]\tvalid_0's auc: 0.725087\tvalid_0's l2: 0.167526\n",
            "[5]\tvalid_0's auc: 0.724959\tvalid_0's l2: 0.166189\n",
            "[6]\tvalid_0's auc: 0.725621\tvalid_0's l2: 0.164986\n",
            "[7]\tvalid_0's auc: 0.727142\tvalid_0's l2: 0.163885\n",
            "[8]\tvalid_0's auc: 0.728577\tvalid_0's l2: 0.162806\n",
            "[9]\tvalid_0's auc: 0.730393\tvalid_0's l2: 0.161819\n",
            "[10]\tvalid_0's auc: 0.731448\tvalid_0's l2: 0.160906\n",
            "[11]\tvalid_0's auc: 0.732238\tvalid_0's l2: 0.160099\n",
            "[12]\tvalid_0's auc: 0.734043\tvalid_0's l2: 0.159331\n",
            "[13]\tvalid_0's auc: 0.734657\tvalid_0's l2: 0.158612\n",
            "[14]\tvalid_0's auc: 0.735504\tvalid_0's l2: 0.157925\n",
            "[15]\tvalid_0's auc: 0.736296\tvalid_0's l2: 0.15727\n",
            "[16]\tvalid_0's auc: 0.737115\tvalid_0's l2: 0.156654\n",
            "[17]\tvalid_0's auc: 0.737804\tvalid_0's l2: 0.156098\n",
            "[18]\tvalid_0's auc: 0.73855\tvalid_0's l2: 0.155556\n",
            "[19]\tvalid_0's auc: 0.739025\tvalid_0's l2: 0.155075\n",
            "[20]\tvalid_0's auc: 0.739221\tvalid_0's l2: 0.154621\n",
            "[21]\tvalid_0's auc: 0.740764\tvalid_0's l2: 0.154171\n",
            "[22]\tvalid_0's auc: 0.741796\tvalid_0's l2: 0.153717\n",
            "[23]\tvalid_0's auc: 0.742438\tvalid_0's l2: 0.153352\n",
            "[24]\tvalid_0's auc: 0.743057\tvalid_0's l2: 0.152974\n",
            "[25]\tvalid_0's auc: 0.743667\tvalid_0's l2: 0.152614\n",
            "[26]\tvalid_0's auc: 0.743933\tvalid_0's l2: 0.152326\n",
            "[27]\tvalid_0's auc: 0.744286\tvalid_0's l2: 0.152032\n",
            "[28]\tvalid_0's auc: 0.7452\tvalid_0's l2: 0.151731\n",
            "[29]\tvalid_0's auc: 0.745545\tvalid_0's l2: 0.151473\n",
            "[30]\tvalid_0's auc: 0.746226\tvalid_0's l2: 0.151236\n",
            "[31]\tvalid_0's auc: 0.746651\tvalid_0's l2: 0.151008\n",
            "[32]\tvalid_0's auc: 0.747172\tvalid_0's l2: 0.150783\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[32]\tvalid_0's auc: 0.747172\tvalid_0's l2: 0.150783\n",
            "Save model...\n",
            "Start predicting...\n",
            "The rmse of prediction is: 0.3883073444614308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWR7DxF3w22p",
        "outputId": "85f9b970-d829-4568-ae39-7b1a21889528"
      },
      "source": [
        "gbm.feature_importance()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 10,   2,  36,  20,  36,  66,  21,  21,  21,   1,  62,   0,  51,\n",
              "         0,  98,   2,  14,   0,   9,   1,   0,   2,   2,  25,   2, 121,\n",
              "        41,  83,  17,  20,  96,   1,   1,   0,   0,  31,   6,   1,   8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWDTbrTew2zB",
        "outputId": "fc9509c7-023e-4700-fd3d-1c4ba02e4ef3"
      },
      "source": [
        "gbm.feature_importance(\"gain\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4959.7851181 ,    70.05719948,  3060.81740379,  1290.30080032,\n",
              "        3068.40098953, 25257.18478775, 18178.44504356,  1466.6282177 ,\n",
              "        1088.65239143,    27.51740074, 12205.77954483,     0.        ,\n",
              "        5568.64053917,     0.        ,  7636.05537224,    66.50469971,\n",
              "         641.64440918,     0.        ,   368.72410583,    54.0868988 ,\n",
              "           0.        ,    75.97299957,   107.39220047,  1072.73800278,\n",
              "         178.64580154,  6770.76226997,  2078.64819145,  4580.94059944,\n",
              "        2108.68519592,  1755.55281258,  9561.59857368,    29.9470005 ,\n",
              "          26.52330017,     0.        ,     0.        ,  1936.1554985 ,\n",
              "         249.021101  ,    32.85879898,   365.58109283])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKGB8m9yw2wP"
      },
      "source": [
        "def ret_feat_impt(gbm):\n",
        "    gain = gbm.feature_importance(\"gain\").reshape(-1, 1) / sum(gbm.feature_importance(\"gain\"))\n",
        "    col = np.array(gbm.feature_name()).reshape(-1, 1)\n",
        "    return sorted(np.column_stack((col, gain)),key=lambda x: x[1],reverse=True)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFhCoPbOw2tc",
        "outputId": "44bffcbd-d767-497e-adb2-d0e767d42563"
      },
      "source": [
        "ret_feat_impt(gbm)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['I6', '0.21784656445602604'], dtype='<U32'),\n",
              " array(['I7', '0.15679149648593121'], dtype='<U32'),\n",
              " array(['I11', '0.10527646539762159'], dtype='<U32'),\n",
              " array(['C18', '0.08247005426333529'], dtype='<U32'),\n",
              " array(['C2', '0.0658619890861652'], dtype='<U32'),\n",
              " array(['C13', '0.058398721459156325'], dtype='<U32'),\n",
              " array(['I13', '0.048030262293402785'], dtype='<U32'),\n",
              " array(['I1', '0.042778803635427326'], dtype='<U32'),\n",
              " array(['C15', '0.039511219478802255'], dtype='<U32'),\n",
              " array(['I5', '0.026465364986651786'], dtype='<U32'),\n",
              " array(['I3', '0.026399955555101454'], dtype='<U32'),\n",
              " array(['C16', '0.018187689139205654'], dtype='<U32'),\n",
              " array(['C14', '0.017928615996830218'], dtype='<U32'),\n",
              " array(['C23', '0.016699597645041024'], dtype='<U32'),\n",
              " array(['C17', '0.015141875555598093'], dtype='<U32'),\n",
              " array(['I8', '0.01264986265272704'], dtype='<U32'),\n",
              " array(['I4', '0.011129015320886438'], dtype='<U32'),\n",
              " array(['I9', '0.009389771083105599'], dtype='<U32'),\n",
              " array(['C11', '0.009252507372862514'], dtype='<U32'),\n",
              " array(['C4', '0.005534268023806238'], dtype='<U32'),\n",
              " array(['C6', '0.003180294255408236'], dtype='<U32'),\n",
              " array(['C26', '0.0031531853519437027'], dtype='<U32'),\n",
              " array(['C24', '0.0021478399823786185'], dtype='<U32'),\n",
              " array(['C12', '0.0015408437024133292'], dtype='<U32'),\n",
              " array(['C10', '0.0009262719546282351'], dtype='<U32'),\n",
              " array(['C9', '0.000655277184984233'], dtype='<U32'),\n",
              " array(['I2', '0.0006042526255391415'], dtype='<U32'),\n",
              " array(['C3', '0.000573611844435768'], dtype='<U32'),\n",
              " array(['C7', '0.00046650666673602375'], dtype='<U32'),\n",
              " array(['C25', '0.0002834114938087318'], dtype='<U32'),\n",
              " array(['C19', '0.00025829684623533943'], dtype='<U32'),\n",
              " array(['I10', '0.00023734122645448818'], dtype='<U32'),\n",
              " array(['C20', '0.00022876697735008426'], dtype='<U32'),\n",
              " array(['I12', '0.0'], dtype='<U32'),\n",
              " array(['C1', '0.0'], dtype='<U32'),\n",
              " array(['C5', '0.0'], dtype='<U32'),\n",
              " array(['C8', '0.0'], dtype='<U32'),\n",
              " array(['C21', '0.0'], dtype='<U32'),\n",
              " array(['C22', '0.0'], dtype='<U32')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzquMp2hhNf"
      },
      "source": [
        "dump = gbm.dump_model()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGZ0CV7RhhKY"
      },
      "source": [
        "save_params_with_name((gbm, dump), 'gbm_dump')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8vEMz19WCem"
      },
      "source": [
        "gbm, dump = load_params_with_name('gbm_dump')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4xL7qpmWCcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U7hEfcdWCYT"
      },
      "source": [
        "def generat_lgb2fm_data(outdir, gbm, dump, tr_path, va_path, te_path, _sep = '\\t'):\n",
        "    with open(os.path.join(outdir, 'train_lgb2fm.txt'), 'w') as out_train:\n",
        "        with open(os.path.join(outdir, 'valid_lgb2fm.txt'), 'w') as out_valid:\n",
        "            with open(os.path.join(outdir, 'test_lgb2fm.txt'), 'w') as out_test:\n",
        "                df_train_ = pd.read_csv(tr_path, header=None, sep=_sep)\n",
        "                df_valid_ = pd.read_csv(va_path, header=None, sep=_sep)\n",
        "                df_test_= pd.read_csv(te_path, header=None, sep=_sep)\n",
        "\n",
        "                y_train_ = df_train_[0].values\n",
        "                y_valid_ = df_valid_[0].values                \n",
        "\n",
        "                X_train_ = df_train_.drop(0, axis=1).values\n",
        "                X_valid_ = df_valid_.drop(0, axis=1).values\n",
        "                X_test_= df_test_.values\n",
        "   \n",
        "                train_leaves= gbm.predict(X_train_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
        "                valid_leaves= gbm.predict(X_valid_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
        "                test_leaves= gbm.predict(X_test_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
        "\n",
        "                tree_info = dump['tree_info']\n",
        "                tree_counts = len(tree_info)\n",
        "                for i in range(tree_counts):\n",
        "                    train_leaves[:, i] = train_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
        "                    valid_leaves[:, i] = valid_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
        "                    test_leaves[:, i] = test_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
        "#                     print(train_leaves[:, i])\n",
        "#                     print(tree_info[i]['num_leaves'])\n",
        "\n",
        "                for idx in range(len(y_train_)):            \n",
        "                    out_train.write((str(y_train_[idx]) + '\\t'))\n",
        "                    out_train.write('\\t'.join(\n",
        "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(train_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
        "                    \n",
        "                for idx in range(len(y_valid_)):                   \n",
        "                    out_valid.write((str(y_valid_[idx]) + '\\t'))\n",
        "                    out_valid.write('\\t'.join(\n",
        "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(valid_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
        "                    \n",
        "                for idx in range(len(X_test_)):                   \n",
        "                    out_test.write('\\t'.join(\n",
        "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(test_leaves[idx]) if float(val) != 0 ]) + '\\n')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkOlUSRCWCV8"
      },
      "source": [
        "generat_lgb2fm_data('./data', gbm, dump, './data/train_lgb.txt', './data/valid_lgb.txt', './data/test_lgb.txt', '\\t')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgi4np0B3UY3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbIEdyCX3UVn",
        "outputId": "4a05bbcf-216e-4080-fd92-d5e673d3fa6c"
      },
      "source": [
        "cmd = 'kaggle_criteo_ctr_challenge-/libfm/libfm/bin/libFM -task c -train ./data/train_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 64 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -save_model fm_model'\n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKvgAApi3USX",
        "outputId": "1a4c5fcd-5d7c-499b-e869-14ff6b4aa421"
      },
      "source": [
        "cmd = 'kaggle_criteo_ctr_challenge-/libfm/libfm/bin/libFM -task c -train ./data/train_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix tr'\n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M00X5ply3UPB",
        "outputId": "d11a9585-a7f4-4672-b0ca-4e69d940530a"
      },
      "source": [
        "cmd = 'kaggle_criteo_ctr_challenge-/libfm/libfm/bin/libFM -task c -train ./data/valid_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix va'\n",
        "os.popen(cmd).readlines()\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00V4Jrrp3ULy",
        "outputId": "145f6ebe-65ca-41a8-ce8c-638f9a153e36"
      },
      "source": [
        "cmd = 'kaggle_criteo_ctr_challenge-/libfm/libfm/bin/libFM -task c -train ./data/test_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix te -test2predict true'\n",
        "os.popen(cmd).readlines()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revUPrNS3UIs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHxm_yPs3UFq"
      },
      "source": [
        "embed_dim = 32\n",
        "sparse_max = 30000 # sparse_feature_dim = 117568\n",
        "sparse_dim = 26\n",
        "dense_dim = 13\n",
        "out_dim = 400"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnyJ_ZtP3UDA"
      },
      "source": [
        "def get_batches(Xs, ys, batch_size):\n",
        "    for start in range(0, len(Xs), batch_size):\n",
        "        end = min(start + batch_size, len(Xs))\n",
        "        yield Xs[start:end], ys[start:end]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE5NtvBe3T_H"
      },
      "source": [
        "def get_batches_downsample(Xs, ys, batch_size):\n",
        "    ind_0 = ys==0\n",
        "    ind_1 = ys==1\n",
        "    Xs_0 = Xs[ind_0]\n",
        "    ys_0 = ys[ind_0]\n",
        "    Xs_1 = Xs[ind_1]\n",
        "    ys_1 = ys[ind_1]\n",
        "    sampling_ind = np.random.permutation(Xs_0.shape[0])[:Xs_1.shape[0]]\n",
        "    Xs_0_sampling = Xs_0[sampling_ind]\n",
        "    ys_0_sampling = ys_0[sampling_ind]\n",
        "    Xs_downsampled = np.concatenate((Xs_0_sampling, Xs_1))\n",
        "    ys_downsampled = np.concatenate((ys_0_sampling, ys_1))\n",
        "    downsampled_ind = np.random.permutation(Xs_downsampled.shape[0])\n",
        "    Xs_downsampled = Xs_downsampled[downsampled_ind]\n",
        "    ys_downsampled = ys_downsampled[downsampled_ind]\n",
        "    for start in range(0, len(Xs_downsampled), batch_size):\n",
        "        end = min(start + batch_size, len(Xs_downsampled))\n",
        "        yield Xs_downsampled[start:end], ys_downsampled[start:end]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-uSNdvc3T7j"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "import time\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import log_loss\n",
        "# from sklearn.learning_curve import learning_curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import metrics as sk_metrics\n",
        "\n",
        "MODEL_DIR = \"./models\"\n",
        "\n",
        "\n",
        "class ctr_network(object):\n",
        "    def __init__(self, batch_size=32):\n",
        "        self.batch_size = batch_size\n",
        "        self.best_loss = 9999\n",
        "\n",
        "        self.losses = {'train': [], 'test': []}\n",
        "        self.pred_lst = []\n",
        "        self.test_y_lst = []\n",
        "        \n",
        "        # 定义输入\n",
        "        dense_input = tf.keras.layers.Input(shape=(dense_dim,), name='dense_input')\n",
        "        sparse_input = tf.keras.layers.Input(shape=(sparse_dim,), name='sparse_input')\n",
        "        FFM_input = tf.keras.layers.Input(shape=(1,), name='FFM_input')\n",
        "        FM_input = tf.keras.layers.Input(shape=(1,), name='FM_input')\n",
        "\n",
        "        # 输入类别特征，从嵌入层获得嵌入向量\n",
        "        sparse_embed_layer = tf.keras.layers.Embedding(sparse_max, embed_dim, input_length=sparse_dim)(sparse_input)\n",
        "        sparse_embed_layer = tf.keras.layers.Reshape([sparse_dim * embed_dim])(sparse_embed_layer)\n",
        "\n",
        "        # 输入数值特征，和嵌入向量链接在一起经过三层全连接层\n",
        "        input_combine_layer = tf.keras.layers.concatenate([dense_input, sparse_embed_layer])  # (?, 845 = 832 + 13)\n",
        "        fc1_layer = tf.keras.layers.Dense(out_dim, name=\"fc1_layer\", activation='relu')(input_combine_layer)\n",
        "        fc2_layer = tf.keras.layers.Dense(out_dim, name=\"fc2_layer\", activation='relu')(fc1_layer)\n",
        "        fc3_layer = tf.keras.layers.Dense(out_dim, name=\"fc3_layer\", activation='relu')(fc2_layer)\n",
        "\n",
        "        ffm_fc_layer = tf.keras.layers.Dense(1, name=\"ffm_fc_layer\")(FFM_input)\n",
        "        fm_fc_layer = tf.keras.layers.Dense(1, name=\"fm_fc_layer\")(FM_input)\n",
        "        feature_combine_layer = tf.keras.layers.concatenate([ffm_fc_layer, fm_fc_layer, fc3_layer], 1)  # (?, 402)\n",
        "\n",
        "        logits_output = tf.keras.layers.Dense(1, name=\"logits_layer\", activation='sigmoid')(feature_combine_layer)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=[dense_input, sparse_input, FFM_input, FM_input], outputs=[logits_output])\n",
        "        self.model.summary()\n",
        "\n",
        "        self.optimizer = tf.compat.v1.train.FtrlOptimizer(0.01)  # tf.keras.optimizers.Adam(0.01)\n",
        "        self.ComputeLoss = tf.keras.losses.LogLoss()\n",
        "\n",
        "        if tf.io.gfile.exists(MODEL_DIR):\n",
        "            #             print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
        "            #             tf.io.gfile.rmtree(MODEL_DIR)\n",
        "            pass\n",
        "        else:\n",
        "            tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "        train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
        "        test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
        "\n",
        "#         self.train_summary_writer = summary_ops_v2.create_file_writer(train_dir, flush_millis=10000)\n",
        "#         self.test_summary_writer = summary_ops_v2.create_file_writer(test_dir, flush_millis=10000, name='test')\n",
        "\n",
        "        checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "        self.checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "        self.checkpoint = tf.train.Checkpoint(model=self.model, optimizer=self.optimizer)\n",
        "\n",
        "        # Restore variables on creation if a checkpoint exists.\n",
        "        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "    def compute_metrics(self, labels, pred):\n",
        "        correct_prediction = tf.equal(tf.keras.backend.cast(pred > 0.5, 'float32'), labels)\n",
        "        accuracy = tf.reduce_mean(tf.keras.backend.cast(correct_prediction, 'float32'), name=\"accuracy\")\n",
        "        return accuracy  \n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x, y):\n",
        "        # Record the operations used to compute the loss, so that the gradient\n",
        "        # of the loss with respect to the variables can be computed.\n",
        "        metrics = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = self.model([x[0],\n",
        "                               x[1],\n",
        "                               x[2],\n",
        "                               x[3]], training=True)\n",
        "            loss = self.ComputeLoss(y, pred)\n",
        "            metrics = self.compute_metrics(y, pred)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss, metrics, pred\n",
        "\n",
        "    def training(self, train_dataset, test_dataset, downsample_flg=True, epochs=1, log_freq=50):\n",
        "\n",
        "        train_X, train_y = train_dataset\n",
        "\n",
        "        for epoch_i in range(epochs):\n",
        "            if downsample_flg:\n",
        "                train_batches = get_batches_downsample(train_X, train_y, self.batch_size)\n",
        "                batch_num = (len(train_y[train_y==1])*2 // self.batch_size)\n",
        "            else:\n",
        "                train_batches = get_batches(train_X, train_y, self.batch_size)\n",
        "                batch_num = len(train_X) // self.batch_size\n",
        "\n",
        "            train_start = time.time()\n",
        "#             with self.train_summary_writer.as_default():\n",
        "            if True:\n",
        "                start = time.time()\n",
        "                # Metrics are stateful. They accumulate values and return a cumulative\n",
        "                # result when you call .result(). Clear accumulated values with .reset_states()\n",
        "                avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "                avg_acc = tf.keras.metrics.Mean('acc', dtype=tf.float32)\n",
        "                avg_auc = tf.keras.metrics.Mean('auc', dtype=tf.float32)\n",
        "\n",
        "                # Datasets can be iterated over like any other Python iterable.\n",
        "                for batch_i in range(batch_num):\n",
        "                    x, y = next(train_batches)\n",
        "                    if len(x) < self.batch_size:\n",
        "                        break\n",
        "                    \n",
        "                    loss, metrics, pred = self.train_step([x.take([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 1),\n",
        "                               x.take(\n",
        "                                   [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
        "                                    36, 37, 38, 39, 40], 1),\n",
        "                               np.reshape(x.take(0, 1), [self.batch_size, 1]),\n",
        "                               np.reshape(x.take(1, 1), [self.batch_size, 1])], np.expand_dims(y, 1))\n",
        "                    avg_loss(loss)\n",
        "                    avg_acc(metrics)\n",
        "\n",
        "                    prediction = tf.reshape(pred, y.shape)\n",
        "                    self.losses['train'].append(loss)\n",
        "\n",
        "                    if (np.mean(y) != 0):\n",
        "                        auc = sk_metrics.roc_auc_score(y, prediction)\n",
        "                    else:\n",
        "                        auc = -1\n",
        "\n",
        "                    avg_auc(auc)\n",
        "                    if tf.equal((epoch_i * (batch_num) + batch_i) % log_freq, 0):\n",
        "#                         summary_ops_v2.scalar('loss', avg_loss.result(), step=self.optimizer.iterations)\n",
        "                        #                         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=self.optimizer.iterations)\n",
        "#                         summary_ops_v2.scalar('acc', avg_acc.result(), step=self.optimizer.iterations)\n",
        "\n",
        "                        rate = log_freq / (time.time() - start)\n",
        "                        \n",
        "                        print('Epoch {:>3} Batch {:>4}/{} Loss: {:0.6f} acc: {:0.6f} auc = {} ({} steps/sec)'.format(\n",
        "                            epoch_i, batch_i, batch_num, avg_loss.result(), (avg_acc.result()), avg_auc.result(), rate))\n",
        "\n",
        "                        avg_auc.reset_states()\n",
        "                        avg_loss.reset_states()\n",
        "                        \n",
        "                        avg_acc.reset_states()\n",
        "                        start = time.time()\n",
        "\n",
        "            train_end = time.time()\n",
        "            print('\\nTrain time for epoch #{} : {}'.format(epoch_i + 1, train_end - train_start))\n",
        "#             with self.test_summary_writer.as_default():\n",
        "            self.testing(test_dataset)\n",
        "            # self.checkpoint.save(self.checkpoint_prefix)\n",
        "        self.export_path = os.path.join(MODEL_DIR, 'export')\n",
        "        tf.saved_model.save(self.model, self.export_path)\n",
        "\n",
        "    def testing(self, test_dataset):\n",
        "        test_X, test_y = test_dataset\n",
        "        test_batches = get_batches(test_X, test_y, self.batch_size)\n",
        "\n",
        "        \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "        avg_acc = tf.keras.metrics.Mean('acc', dtype=tf.float32)\n",
        "        avg_auc = tf.keras.metrics.Mean('auc', dtype=tf.float32)\n",
        "        avg_prediction = tf.keras.metrics.Mean('prediction', dtype=tf.float32)\n",
        "\n",
        "        self.pred_lst=[]\n",
        "        self.test_y_lst=[]\n",
        "        \n",
        "        batch_num = (len(test_X) // self.batch_size)\n",
        "        for batch_i in range(batch_num):\n",
        "            x, y = next(test_batches)\n",
        "            if len(x) < self.batch_size:\n",
        "                break\n",
        "            \n",
        "            pred = self.model([x.take([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 1),\n",
        "                               x.take(\n",
        "                                   [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
        "                                    36, 37, 38, 39, 40], 1),\n",
        "                               np.reshape(x.take(0, 1), [self.batch_size, 1]),\n",
        "                               np.reshape(x.take(1, 1), [self.batch_size, 1])], training=False)\n",
        "            test_loss = self.ComputeLoss(np.expand_dims(y, 1), pred)\n",
        "            avg_loss(test_loss)\n",
        "            acc = self.compute_metrics(np.expand_dims(y, 1), pred)\n",
        "            avg_acc(acc)\n",
        "\n",
        "            # 保存测试损失和准确率\n",
        "            prediction = tf.reshape(pred, y.shape)\n",
        "            avg_prediction(prediction)\n",
        "            self.losses['test'].append(test_loss)\n",
        "\n",
        "            self.pred_lst.append(prediction)\n",
        "            self.test_y_lst.append(y)\n",
        "\n",
        "            if (np.mean(y) != 0):\n",
        "                auc = sk_metrics.roc_auc_score(y, prediction)\n",
        "            else:\n",
        "                auc = -1\n",
        "            avg_auc(auc)\n",
        "\n",
        "        self.pred_lst = np.concatenate([val for val in self.pred_lst])\n",
        "        self.test_y_lst = np.concatenate([val for val in self.test_y_lst])\n",
        "        print('Model test set loss: {:0.6f}  acc: {:0.6f}  auc = {} prediction = {}'.format(\n",
        "            avg_loss.result(), avg_acc.result(), avg_auc.result(), avg_prediction.result()))\n",
        "        print(sk_metrics.classification_report(self.test_y_lst, tf.keras.backend.cast((self.pred_lst) > 0.5, 'float32')))\n",
        "#         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
        "#         summary_ops_v2.scalar('acc', avg_acc.result(), step=step_num)\n",
        "\n",
        "        if avg_loss.result() < self.best_loss:\n",
        "            self.best_loss = avg_loss.result()\n",
        "            print(\"best loss = {}\".format(self.best_loss))\n",
        "            self.checkpoint.save(self.checkpoint_prefix)\n",
        "\n",
        "    def predict_click(self, x, axis = 0):\n",
        "        clicked = self.model([np.reshape(x.take([2,3,4,5,6,7,8,9,10,11,12,13,14],axis), [1 if axis == 0 else len(x.take([2,3,4,5,6,7,8,9,10,11,12,13,14],axis)), 13]),\n",
        "                               np.reshape(x.take([15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40],axis), [1 if axis == 0 else len(x.take([15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40],axis)), 26]),\n",
        "                               np.reshape(x.take(0,axis), [1 if axis == 0 else len(x.take(0,axis)), 1]),\n",
        "                               np.reshape(x.take(1,axis), [1 if axis == 0 else len(x.take(0,axis)), 1])])\n",
        "\n",
        "        return (np.int32(np.array(clicked) > 0.5))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C1xjmozWCS1"
      },
      "source": [
        "# Number of Epochs\n",
        "num_epochs = 1\n",
        "# Batch Size\n",
        "batch_size = 32\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = 0.01\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 25\n",
        "\n",
        "save_dir = './save'\n",
        "\n",
        "ffm_tr_out_path = './tr_ffm.out.logit'\n",
        "ffm_va_out_path = './va_ffm.out.logit'\n",
        "fm_tr_out_path = './tr.fm.logits'\n",
        "fm_va_out_path = './va.fm.logits'\n",
        "train_path = './data/train.txt'\n",
        "valid_path = './data/valid.txt'"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaL-iHDyWCPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "eb29a981-3486-468a-849b-22af0c7694e8"
      },
      "source": [
        "ffm_train = pd.read_csv(ffm_tr_out_path, header=None)    \n",
        "ffm_train = ffm_train[0].values\n",
        "\n",
        "ffm_valid = pd.read_csv(ffm_va_out_path, header=None)    \n",
        "ffm_valid = ffm_valid[0].values"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-ed5b7498655c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mffm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffm_tr_out_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mffm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffm_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mffm_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffm_va_out_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mffm_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffm_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tr_ffm.out.logit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfT4y7klWBWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}