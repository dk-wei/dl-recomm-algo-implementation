{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBDT_LR (CTR预估 + Churn rate预测).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwEXgSRuVN0bk4il9wuqvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/recommendation-algo/blob/main/GBDT_LR_(CTR%E9%A2%84%E4%BC%B0_%2B_Churn_rate%E9%A2%84%E6%B5%8B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-yL44EP4Mr0"
      },
      "source": [
        "![](https://pic4.zhimg.com/80/v2-7fe5861dd26ef82c3e79f25f46c4ce83_1440w.jpg)\n",
        "\n",
        "图中Tree1、Tree2为通过GBDT模型学出来的两颗树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。由于树的每条路径，是通过最小化均方差等方法最终分割出来的有区分性路径，根据该路径得到的特征、特征组合都相对有区分性，效果理论上不会亚于人工经验的处理方式。\n",
        "\n",
        "GBDT模型的特点，非常适合用来挖掘有效的特征、特征组合。业界不仅GBDT+LR融合有实践，GBDT+FM也有实践，2014 Kaggle CTR竞赛冠军就是使用GBDT+FM，可见，使用GBDT融合其它模型是非常值得尝试的思路。\n",
        "\n",
        "调研了Facebook、Kaggle竞赛关于GBDT建树的细节，发现两个关键点：采用ensemble决策树而非单颗树；建树采用GBDT而非RF（Random Forests）。解读如下：\n",
        "\n",
        "1） 为什么建树采用ensemble决策树？\n",
        "\n",
        "  一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在\n",
        "  学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，\n",
        "  多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。\n",
        "\n",
        "  2） 为什么建树采用GBDT而非RF？\n",
        "\n",
        "  RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，\n",
        "  主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，\n",
        "  思路更加合理，这应该也是用GBDT的原因。\n",
        "\n",
        "  **GBDT + LR不只是可以用来做CTR预估(根据LR的概率排序)，也可以应用到寻常的Classification模型中，下文我们给出了两例**："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4vBObq8738s"
      },
      "source": [
        "# Case 1: Porto Seguro’s Safe Driver Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQT6ZGZgAKRN"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqKgbxxkEmaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05eaa597-f438-4132-a280-43e85b16d2dd"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "TRAINING_PATH = '/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/train.csv'\n",
        "TESTING_PATH  = '/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/test.csv'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCr0F5DPAYw_",
        "outputId": "3d3eb646-5a63-4d34-cb61-497242f85bf0"
      },
      "source": [
        "pd.read_csv(TRAINING_PATH).shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(595212, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HifC0db_-zKO",
        "outputId": "fba56bc6-6553-4755-a62c-6e0cadbea85e"
      },
      "source": [
        "print('Load data...')\n",
        "df_train = pd.read_csv(TRAINING_PATH).iloc[:10000]\n",
        "df_test  = pd.read_csv(TRAINING_PATH).iloc[10000:12000]\n",
        "\n",
        "NUMERIC_COLS = [\n",
        "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
        "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
        "]\n",
        "\n",
        "\n",
        "print(df_train.head(3))\n",
        "print(df_test.head(3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "   id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "0   7       0          2  ...               0               0               1\n",
            "1   9       0          1  ...               0               1               0\n",
            "2  13       0          5  ...               0               1               0\n",
            "\n",
            "[3 rows x 59 columns]\n",
            "          id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "10000  25242       0          0  ...               1               0               0\n",
            "10001  25246       0          1  ...               1               1               0\n",
            "10002  25247       0          0  ...               1               0               0\n",
            "\n",
            "[3 rows x 59 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnQTwQH8AouZ"
      },
      "source": [
        "y_train = df_train['target']  # training label\n",
        "y_test = df_test['target']  # testing label\n",
        "X_train = df_train[NUMERIC_COLS]  # training dataset\n",
        "X_test = df_test[NUMERIC_COLS]  # testing dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7og5BCzvOmg"
      },
      "source": [
        "训练GBDT模型\n",
        "\n",
        "本文使用lightgbm包来训练我们的GBDT模型，每个case训练100 trees，每棵树有64个叶子结点 (leaves)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYuspr8zEmNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a1943b-3ac9-47e8-f301-13c6552601ca"
      },
      "source": [
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'binary_logloss'},\n",
        "    'num_trees': 100,  # 每个case总共100棵树\n",
        "    'num_leaves': 64,   # 每棵树有64个leaves\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# number of leaves,will be used in feature transformation\n",
        "num_leaf = 64\n",
        "\n",
        "print('Start training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=100,\n",
        "                valid_sets=lgb_train,\n",
        "                verbose_eval = False\n",
        "                )\n",
        "\n",
        "print('Save model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Save model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7efdc6c95cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91cb0M_GvuGG"
      },
      "source": [
        "特征转换\n",
        "\n",
        "在训练得到100棵树之后，我们需要得到的不是GBDT的预测结果，而是每一条训练数据落在了每棵树的哪个叶子结点上，因此需要使用下面的语句："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coagmzackiRb",
        "outputId": "2b0f4813-b02b-4f9e-96c7-7d77a20ba3b0"
      },
      "source": [
        "print('Start predicting...')\n",
        "# predict and get data on leaves, training data\n",
        "y_pred = gbm.predict(X_train, pred_leaf=True)\n",
        "\n",
        "print(np.array(y_pred).shape)\n",
        "print(y_pred[0])\n",
        "print(y_pred[0].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start predicting...\n",
            "(10000, 100)\n",
            "[28 59 17 50 31 22 18 40 22 40 61  7  7  7 53  3 47 22 23 35 41 34 33 43\n",
            " 50 46 19 42 44 48 42 30 10  9  9 63 21 61 46 58 23 37 50  6 56 35 34 38\n",
            " 36 40 34 40 30 30 36 52 29 54 35 62  0 48 52 37 55  1 57 61 45 57 36 53\n",
            " 36 36 14 54 56 40 38 38  1  1 30  1 38 62 25 31 29 23 29 40 18 41 40  1\n",
            " 42 52 48 45]\n",
            "(100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_nUyjsvx07"
      },
      "source": [
        "打印上面结果的输出，可以看到shape是[10000,100]，即`[data size * #trees per case]`\n",
        "\n",
        "然后我们需要将每棵树的特征进行one-hot处理，如前面所说，总共100棵树，假设第一棵树落在28号leaf node上，那我们需要建立一个64维的向量(因为有64棵树)，除43维之外全部都是0。因此用于LR训练的特征维数共`[num_trees * num_leaves]`，也就是100个64维的one-hot vector。\n",
        "\n",
        "这样就为每个data point创建了`[100*64]`的新特征向量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko_tgdLSEmKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d0a90f-2a0c-42b0-9694-7f2d537cc2c3"
      },
      "source": [
        "print('Writing transformed training data')\n",
        "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
        "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
        "\n",
        "print(transformed_training_matrix[0].shape)   \n",
        "\n",
        "for i in range(0, len(y_pred)):\n",
        "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
        "    transformed_training_matrix[i][temp] += 1\n",
        "\n",
        "\n",
        "y_pred = gbm.predict(X_test, pred_leaf=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing transformed training data\n",
            "(6400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBGpaPL2mup"
      },
      "source": [
        "每个data point transform为为`[100*64]`的新特征向量。当然，对于测试集也要进行同样的处理.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwqARpXTEmHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765dcefb-4c9d-4564-8966-b5b6201974ee"
      },
      "source": [
        "print('Writing transformed testing data')\n",
        "transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
        "for i in range(0, len(y_pred)):\n",
        "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
        "    transformed_testing_matrix[i][temp] += 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing transformed testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxePwtlA3ITR"
      },
      "source": [
        "LR训练\n",
        "\n",
        "然后我们可以用转换后的训练集特征和label训练我们的LR模型，并对测试集进行测试："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SXMfyDQ3H9v",
        "outputId": "2efb8448-7f6b-4586-b6e8-2f043aa124f5"
      },
      "source": [
        "lm = LogisticRegression(penalty='l2',C=0.05) # logestic model construction\n",
        "lm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
        "y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
        "\n",
        "print(y_pred_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.97742378 0.02257622]\n",
            " [0.98815137 0.01184863]\n",
            " [0.98046795 0.01953205]\n",
            " ...\n",
            " [0.98790653 0.01209347]\n",
            " [0.99404478 0.00595522]\n",
            " [0.97347545 0.02652455]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izxaS_6y3WNM"
      },
      "source": [
        "我们这里得到的不是简单的类别，而是每个类别的概率, 我们需要对这样的类别概率进行排序，得到我们的TopN。\n",
        "\n",
        "效果评价\n",
        "在Facebook的paper中，模型使用NE(Normalized Cross-Entropy)，进行评价，计算公式如下:\n",
        "\n",
        "![](https://pic2.zhimg.com/80/v2-b2a88a7874e01b316d10a31aa3863171_1440w.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD2-9FsmEmEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059261d5-abe9-4a53-d52f-61f730fbc336"
      },
      "source": [
        "NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))\n",
        "print(\"Normalized Cross Entropy \" + str(NE))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized Cross Entropy 2.1481980203420465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQzfr6vEmAu"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMPKvdDeEl9-"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBziDP677bL"
      },
      "source": [
        "# Case 2: 基于LR+XGBoost预估电信客户流失\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEHJYGPuEl6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c2e2f1-f361-4a0e-fccc-4e31a5cb2935"
      },
      "source": [
        "# -*-coding:utf-8-*-\n",
        "\"\"\"\n",
        "    Author: Alan\n",
        "    Desc:\n",
        "        GBDT+LR模型 电信客户流失预测\n",
        "\"\"\"\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class ChurnPredWithGBDTAndLR:\n",
        "    def __init__(self):\n",
        "        self.file = \"/content/gdrive/MyDrive/扬FAANG起航/单项准备/(GBDT+LR) Porto Seguro’s Safe Driver Prediction/new_churn.csv\"\n",
        "        self.data = self.load_data()\n",
        "        self.train, self.test = self.split()\n",
        "\n",
        "    # 加载数据\n",
        "    def load_data(self):\n",
        "        return pd.read_csv(self.file)\n",
        "\n",
        "    # 拆分数据集\n",
        "    def split(self):\n",
        "        train, test = train_test_split(self.data, test_size=0.1, random_state=40)\n",
        "        return train, test\n",
        "\n",
        "    # 模型训练\n",
        "    def train_model(self):\n",
        "        lable = \"Churn\"\n",
        "        ID = \"customerID\"\n",
        "        x_columns = [x for x in self.train.columns if x not in [lable, ID]]\n",
        "        x_train = self.train[x_columns]\n",
        "        y_train = self.train[lable]\n",
        "\n",
        "        # 创建gbdt模型 并训练\n",
        "        gbdt = GradientBoostingClassifier()\n",
        "        gbdt.fit(x_train, y_train)\n",
        "\n",
        "        # 模型融合\n",
        "        gbdt_lr = LogisticRegression()\n",
        "        enc = OneHotEncoder()\n",
        "        print(gbdt.apply(x_train).shape)\n",
        "        print(gbdt.apply(x_train).reshape(-1,100).shape)\n",
        "\n",
        "        # 100为n_estimators，迭代次数\n",
        "        enc.fit(gbdt.apply(x_train).reshape(-1,100))\n",
        "        gbdt_lr.fit(enc.transform(gbdt.apply(x_train).reshape(-1,100)),y_train)\n",
        "\n",
        "        return enc, gbdt, gbdt_lr\n",
        "\n",
        "    # 效果评估\n",
        "    def evaluate(self,enc,gbdt,gbdt_lr):\n",
        "        lable = \"Churn\"\n",
        "        ID = \"customerID\"\n",
        "        x_columns = [x for x in self.test.columns if x not in [lable, ID]]\n",
        "        x_test = self.test[x_columns]\n",
        "        y_test = self.test[lable]\n",
        "\n",
        "        # gbdt 模型效果评估\n",
        "        gbdt_y_pred = gbdt.predict_proba(x_test)\n",
        "        new_gbdt_y_pred = list()\n",
        "        for y in gbdt_y_pred:\n",
        "            # y[0] 表示样本label=0的概率 y[1]表示样本label=1的概率\n",
        "            new_gbdt_y_pred.append(1 if y[1] > 0.5 else 0)\n",
        "        print(\"GBDT-MSE: %.4f\" % mean_squared_error(y_test, new_gbdt_y_pred))\n",
        "        print(\"GBDT-Accuracy : %.4g\" % metrics.accuracy_score(y_test.values, new_gbdt_y_pred))\n",
        "        print(\"GBDT-AUC Score : %.4g\" % metrics.roc_auc_score(y_test.values, new_gbdt_y_pred))\n",
        "\n",
        "        gbdt_lr_y_pred = gbdt_lr.predict_proba(enc.transform(gbdt.apply(x_test).reshape(-1,100)))\n",
        "        new_gbdt_lr_y_pred = list()\n",
        "        for y in gbdt_lr_y_pred:\n",
        "            # y[0] 表示样本label=0的概率 y[1]表示样本label=1的概率\n",
        "            new_gbdt_lr_y_pred.append(1 if y[1] > 0.5 else 0)\n",
        "        print(\"GBDT_LR-MSE: %.4f\" % mean_squared_error(y_test, new_gbdt_lr_y_pred))\n",
        "        print(\"GBDT_LR-Accuracy : %.4g\" % metrics.accuracy_score(y_test.values, new_gbdt_lr_y_pred))\n",
        "        print(\"GBDT_LR-AUC Score : %.4g\" % metrics.roc_auc_score(y_test.values, new_gbdt_lr_y_pred))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pred = ChurnPredWithGBDTAndLR()\n",
        "    enc, gbdt, gbdt_lr = pred.train_model()\n",
        "    pred.evaluate(enc, gbdt,gbdt_lr)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6338, 100, 1)\n",
            "(6338, 100)\n",
            "GBDT-MSE: 0.2199\n",
            "GBDT-Accuracy : 0.7801\n",
            "GBDT-AUC Score : 0.7058\n",
            "GBDT_LR-MSE: 0.2638\n",
            "GBDT_LR-Accuracy : 0.7362\n",
            "GBDT_LR-AUC Score : 0.6647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43WH0YZg77Sd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}